{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statlog Vehicle Silhouettes Project\n",
    "By : Shashidhar. B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "The purpose of the case study is to classify a given silhouette as one of four different types of vehicle, using a set of features extracted from the silhouette. The vehicle may be viewed from one of many different angles.\n",
    "\n",
    "Four \"Corgie\" model vehicles were used for the experiment: a double decker bus, Cheverolet van, Saab 9000 and an Opel Manta 400 cars. This particular combination of vehicles was chosen with the expectation that the bus, van and either one of the cars would be readily distinguishable, but it would be more difficult to distinguish between the cars.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source\n",
    "The original dataset is available at UCI Machine Learning Repository and can be downloaded from this address: https://archive.ics.uci.edu/ml/datasets/Statlog+(Vehicle+Silhouettes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "1. Data pre-processing - Understand the data and treat missing values.\n",
    "\n",
    "2. Understanding the attributes - Find relationship between different attributes (Independent variables) and choose carefully which all attributes have to be a part of the analysis and why?\n",
    "\n",
    "3. Use PCA from scikit learn and elbow plot to find out reduced number of dimension (which covers more than 95% of the variance).\n",
    "\n",
    "4. Use Support vector machines and use grid search (try C values - 0.01, 0.05, 0.5, 1 and kernel = linear, rbf) and find out the best hyper parameters and do cross validation to find the accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps carried out to achieve objectives\n",
    "1. Import Necessary Libraries and Packages.\n",
    "\n",
    "2. Import the Data.\n",
    "\n",
    "3. EDA [Eploratory Data Analysis].\n",
    "\n",
    "4. Data Cleaning and Data Visualisation.\n",
    "   \n",
    "5. PCA and SVM Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Necessary Libraries and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Numerial Operations numpy(Numerical Python) is used\n",
    "import numpy as np\n",
    "\n",
    "#For Data Analysis pandas(Python Data Analysis Library) is used\n",
    "import pandas as pd\n",
    "\n",
    "#For 2D graphs ploting matplotlib library is used\n",
    "import matplotlib.pyplot as plt\n",
    "#To enable matplotlib to plot in Jupyter Notebook\n",
    "%matplotlib inline\n",
    "\n",
    "#For better visualisation of Statistical Data seaborn library is used\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'vehicle.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a2acc6d2b6e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Reading the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"vehicle.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'vehicle.csv' does not exist"
     ]
    }
   ],
   "source": [
    "#Reading the data\n",
    "data=pd.read_csv(\"vehicle.csv\",sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. EDA [Eploratory Data Analysis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 846-rows and 19-columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only class atribute is of object type and all others are numeric.\n",
    "\n",
    "There are missing values in various columns, lets explore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are null entries as shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are NaN(Not a Number) entries as shown, which is same as null values output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data==0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are zero values entred in column 'skewness_about' and 'skewness_about.1', which are not a missing values but the actual entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include='all').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Cleaning and Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Firstly replacing all the null values with random inputs of their respective IQR[Inter Qurtile Range],i.e.,between 25% and 75%.\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['circularity']=data['circularity'].fillna(random.randrange(40, 49))\n",
    "\n",
    "data['distance_circularity']=data['distance_circularity'].fillna(random.randrange(70, 98))\n",
    "\n",
    "data['radius_ratio']=data['radius_ratio'].fillna(random.randrange(114, 195))\n",
    "\n",
    "data['pr.axis_aspect_ratio']=data['pr.axis_aspect_ratio'].fillna(random.randrange(57, 65))\n",
    "\n",
    "data['scatter_ratio']=data['scatter_ratio'].fillna(random.randrange(147, 198))\n",
    "\n",
    "data['elongatedness']=data['elongatedness'].fillna(random.randrange(33, 46))\n",
    "\n",
    "data['pr.axis_rectangularity']=data['pr.axis_rectangularity'].fillna(random.randrange(19, 23))\n",
    "\n",
    "data['scaled_variance']=data['scaled_variance'].fillna(random.randrange(167, 217))\n",
    "\n",
    "data['scaled_variance.1']=data['scaled_variance.1'].fillna(random.randrange(318, 587))\n",
    "\n",
    "data['scaled_radius_of_gyration']=data['scaled_radius_of_gyration'].fillna(random.randrange(149, 198))\n",
    "\n",
    "data['scaled_radius_of_gyration.1']=data['scaled_radius_of_gyration.1'].fillna(random.randrange(67, 75))\n",
    "\n",
    "data['skewness_about']=data['skewness_about'].fillna(random.randrange(2, 9))\n",
    "\n",
    "data['skewness_about.1']=data['skewness_about.1'].fillna(random.randrange(5, 19))\n",
    "\n",
    "data['skewness_about.2']=data['skewness_about.2'].fillna(random.randrange(184, 193))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, we can see that all the missing values are taken care."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Visualisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,axes=plt.subplots(3,3,figsize=(20,15))\n",
    "sns.boxplot(data['compactness'],ax=axes[0,0])\n",
    "sns.boxplot(data['circularity'],ax=axes[0,1])\n",
    "sns.boxplot(data['distance_circularity'],ax=axes[0,2])\n",
    "sns.boxplot(data['radius_ratio'],ax=axes[1,0])\n",
    "sns.boxplot(data['pr.axis_aspect_ratio'],ax=axes[1,1])\n",
    "sns.boxplot(data['max.length_aspect_ratio'],ax=axes[1,2])\n",
    "sns.boxplot(data['scatter_ratio'],ax=axes[2,0])\n",
    "sns.boxplot(data['elongatedness'],ax=axes[2,1])\n",
    "sns.boxplot(data['pr.axis_rectangularity'],ax=axes[2,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,axes=plt.subplots(3,3,figsize=(20,15))\n",
    "sns.boxplot(data['max.length_rectangularity'],ax=axes[0,0])\n",
    "sns.boxplot(data['scaled_variance'],ax=axes[0,1])\n",
    "sns.boxplot(data['scaled_variance.1'],ax=axes[0,2])\n",
    "sns.boxplot(data['scaled_radius_of_gyration'],ax=axes[1,0])\n",
    "sns.boxplot(data['scaled_radius_of_gyration.1'],ax=axes[1,1])\n",
    "sns.boxplot(data['skewness_about'],ax=axes[1,2])\n",
    "sns.boxplot(data['skewness_about.1'],ax=axes[2,0])\n",
    "sns.boxplot(data['skewness_about.2'],ax=axes[2,1])\n",
    "sns.boxplot(data['hollows_ratio'],ax=axes[2,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data['radius_ratio'])):\n",
    "    if data.loc[i,'radius_ratio']>300:\n",
    "        data.loc[i,'radius_ratio']= random.randrange(114, 195)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data['pr.axis_aspect_ratio'])):\n",
    "    if data.loc[i,'pr.axis_aspect_ratio']>90:\n",
    "        data.loc[i,'pr.axis_aspect_ratio']= random.randrange(57, 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data['max.length_aspect_ratio'])):\n",
    "    if data.loc[i,'max.length_aspect_ratio']>18:\n",
    "        data.loc[i,'max.length_aspect_ratio']= random.randrange(7, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data['max.length_aspect_ratio'])):\n",
    "    if data.loc[i,'max.length_aspect_ratio']<3:\n",
    "        data.loc[i,'max.length_aspect_ratio']= random.randrange(7, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data['scaled_variance'])):\n",
    "    if data.loc[i,'scaled_variance']>300:\n",
    "        data.loc[i,'scaled_variance']= random.randrange(167, 217)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data['scaled_variance.1'])):\n",
    "    if data.loc[i,'scaled_variance.1']>982:\n",
    "        data.loc[i,'scaled_variance.1']= random.randrange(318, 587)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data['scaled_radius_of_gyration.1'])):\n",
    "    if data.loc[i,'scaled_radius_of_gyration.1']>87:\n",
    "        data.loc[i,'scaled_radius_of_gyration.1']= random.randrange(67, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data['skewness_about'])):\n",
    "    if data.loc[i,'skewness_about']>18:\n",
    "        data.loc[i,'skewness_about']= random.randrange(2, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data['skewness_about.1'])):\n",
    "    if data.loc[i,'skewness_about.1']>39:\n",
    "        data.loc[i,'skewness_about.1']= random.randrange(5, 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new=data.drop('class',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "data_scaled=data_new.apply(zscore)\n",
    "data_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new.boxplot(column=['compactness','circularity','distance_circularity','radius_ratio','pr.axis_aspect_ratio','max.length_aspect_ratio','scatter_ratio','elongatedness','pr.axis_rectangularity'],figsize=(20,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new.boxplot(column=['max.length_rectangularity','scaled_variance','scaled_variance.1','scaled_radius_of_gyration','scaled_radius_of_gyration.1','skewness_about','skewness_about.1','skewness_about.2','hollows_ratio'],figsize=(20,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we can see that the data is free from outliers and are on same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see how these data are correlated\n",
    "col_names =data_new.columns\n",
    "corr_matrix = data_new[col_names].corr().abs()\n",
    "plt.figure(figsize = (20,20))\n",
    "cmap = sns.diverging_palette(500, 10, as_cmap=True)\n",
    "sns.heatmap(corr_matrix,annot=True, xticklabels=corr_matrix.columns.values, yticklabels=corr_matrix.columns.values, vmax=.5, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .82})\n",
    "plt.title('Heatmap of Correlation Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can note that, the attributes 'pr.axis_aspect_ratio' is in very bad correlation with all of the other attributes. Hence, this could be droped.\n",
    "\n",
    "The majority of the attributes are moderate-to-high in corelation with each other. Using PCA gives good results.\n",
    "\n",
    "Attributes with high correlation are as follows;\n",
    "\n",
    "1. 'elongatedness' and 'scatter_ratio'\n",
    "\n",
    "2. 'pr.axis_rectangularity' and 'scatter_ratio'\n",
    "\n",
    "3. 'pr.axis_rectangularity' and 'elongatedness'\n",
    "\n",
    "4. 'max.length_rectangularity' and 'circularity'\n",
    " \n",
    "5. 'scaled_variance' and 'scatter_ratio'\n",
    "\n",
    "6. 'scaled_variance' and 'elongatedness'\n",
    "\n",
    "7. 'scaled_variance' and 'pr.axis_rectangularity'\n",
    "\n",
    "8. 'scaled_variance.1' and 'scatter_ratio'\n",
    "\n",
    "9. 'scaled_variance.1' and 'elongatedness'\n",
    "\n",
    "10. 'scaled_variance.1' and 'pr.axis_rectangularity'\n",
    "\n",
    "We have 10 combinations of 7 attributes which are highly co-related (more than 90%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. PCA and SVM Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run PCA;\n",
    "\n",
    "    Dataset should be free from dependent variable.\n",
    "\n",
    "    Attributes should be in same scale to create PCA dimensions.\n",
    "\n",
    "we have already transformed and scaled the data to suit this requirement using z-scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Create covariance matrix\n",
    "\n",
    "cov_matrix = np.cov(data.T)\n",
    "print('Covariance Matrix \\n%s', cov_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2- Get eigen values and eigen vector\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_matrix)\n",
    "print('Eigen Vectors \\n%s', eig_vecs)\n",
    "print('\\n Eigen Values \\n%s', eig_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3- Find variance and cumulative variance by each eigen vector\n",
    "\n",
    "tot = sum(eig_vals)\n",
    "var_exp = [( i /tot ) * 100 for i in sorted(eig_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "print(\"Cumulative Variance Explained\", cum_var_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Eigen values are sorted from highest to lowest variance.\n",
    "\n",
    "We will proceed with 19 components here. But depending on requirement 95% variation, 7 components will do good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(var_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually we can observe that their is steep drop in variance explained with increase in number of PC's.\n",
    "\n",
    "we can note that the Elbow/Knee is found at around 2.\n",
    "\n",
    "Which means that we can use k=2 number of clusters as an optimal value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting \n",
    "plt.figure(figsize=(10 , 5))\n",
    "plt.bar(range(1, eig_vals.size + 1), var_exp, alpha = 0.5, align = 'center', label = 'Individual explained variance')\n",
    "plt.step(range(1, eig_vals.size + 1), cum_var_exp, where='mid', label = 'Cumulative explained variance')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.legend(loc = 'best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA on Raw/Unaltered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using scikit learn PCA here. It does all the above steps and maps data to PCA dimensions in one shot\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# NOTE - we shall generate with all the PCA dimensions firstly\n",
    "\n",
    "pca = PCA(n_components=18)\n",
    "data_full = pca.fit_transform(data_new)\n",
    "pca.fit_transform(data_new).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting to dataframe\n",
    "data_comp_full = pd.DataFrame(pca.components_,columns=list(data_new))\n",
    "\n",
    "#reinserting the droped target variable\n",
    "data_comp_full['class'] = data['class']\n",
    "data_comp_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can note that the Z-Scores have changed.\n",
    "\n",
    "The each Z-Scores here are the combination scores of all the other corresponding attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us check it visually\n",
    "#sns.pairplot(data_comp_full, diag_kind='kde') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# To calculate the accuracy score of the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "target = data_comp_full[\"class\"]\n",
    "features = data_comp_full.drop(\"class\", axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,target, test_size = 0.35, random_state = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is split into Test an Train in the ratio 70:30.\n",
    "\n",
    "The random_state enables us to find teh exact data set to alter for multiple itterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Building a Support Vector Machine on train data\n",
    "svc_model = SVC(C= .1, kernel='linear')\n",
    "svc_model.fit(X_train, y_train)\n",
    "\n",
    "prediction = svc_model .predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gamma is a measure of influence of a data point. It is inverse of distance of influence. C is complexity of the model.\n",
    "\n",
    "lower C value creates simple hyper surface while higher C creates complex surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the accuracy on the training set\n",
    "print(svc_model.score(X_train, y_train))\n",
    "print(svc_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Support Vector Machine on train data\n",
    "svc_model = SVC(kernel='rbf',c=1)\n",
    "svc_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(svc_model.score(X_train, y_train))\n",
    "print(svc_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA and SVM on Reduced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE - we shall generate PC dimensions for raw data (7 dimensions to captured 95% variance)\n",
    "\n",
    "pca = PCA(n_components=7)\n",
    "data_pca = pca.fit_transform(data_new)\n",
    "#pca.fit_transform(data_new).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting to dataframe\n",
    "data_comp_full = pd.DataFrame(data_pca,columns=['PC1','PC2','PC3','PC4','PC5','PC6','PC7'])\n",
    "\n",
    "#reinserting the droped target variable\n",
    "data_comp_full['class'] = data['class']\n",
    "data_comp_full.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us check it visually\n",
    "sns.pairplot(data_comp_full, diag_kind='kde') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the scattering of the data. Hence co-linearity removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let us check again to see if PCA has removed multi colkinierity among new PC dimensions\n",
    "col_names =data_comp_full.columns\n",
    "corr_matrix = data_comp_full[col_names].corr().abs()\n",
    "plt.figure(figsize = (7,7))\n",
    "cmap = sns.diverging_palette(500, 10, as_cmap=True)\n",
    "sns.heatmap(corr_matrix,annot=True, xticklabels=corr_matrix.columns.values, yticklabels=corr_matrix.columns.values, vmax=.5, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .82})\n",
    "plt.title('Heatmap of Correlation Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now multi colinierity is removed and dimensions are also reduced. These 7 PC features capture 95% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# To calculate the accuracy score of the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "target = data_comp_full[\"class\"]\n",
    "features = data_comp_full.drop(\"class\", axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,target, test_size = 0.30, random_state = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is split into Test an Train in the ratio 70:30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Grid Search with C values - 0.01, 0.05, 0.5 and 1 and with Kernal - Linear and RBF and Cross Validation with 5 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SVC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a924468da14e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m params = {'C': [0.01, 0.05, 0.5, 1], \n\u001b[0;32m      2\u001b[0m           'kernel': ['linear','rbf']}\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#Making models with hyper parameters sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SVC' is not defined"
     ]
    }
   ],
   "source": [
    "params = {'C': [0.01, 0.05, 0.5, 1], \n",
    "          'kernel': ['linear','rbf']}\n",
    "model = SVC()\n",
    "#Making models with hyper parameters sets\n",
    "model1 = GridSearchCV(model, param_grid=params, cv=5,n_jobs=-1)\n",
    "#Learning\n",
    "model1.fit(X_train,y_train)\n",
    "#The best hyper parameters set\n",
    "print(\"Best Hyper Parameters:\\n\",model1.best_params_)\n",
    "#Prediction\n",
    "prediction=model1.predict(X_test)\n",
    "#importing the metrics module\n",
    "from sklearn import metrics\n",
    "#evaluation(Accuracy)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(prediction,y_test))\n",
    "#evaluation(Confusion Metrix)\n",
    "print(\"Confusion Matrix:\\n\",metrics.confusion_matrix(prediction,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Confusion matrix we can note that only minmal miss-clasification is found, \n",
    "\n",
    "implying to 92.9 ~ 93% accuracy from kernel= rbf and c=1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
